# Value-Only Training Experiment - å®Ÿè¡ŒçŠ¶æ³

**é–‹å§‹æ—¥æ™‚**: 2026-01-20 23:44 JST
**å®Ÿé¨“å**: Phase 1 - Value-Only Training
**ç†è«–çš„æ ¹æ‹ **: Wang & Emmerich (2019) "Policy or Value?"

---

## å®Ÿé¨“è¨­å®š

### æå¤±é–¢æ•°ï¼ˆâ˜…é‡è¦å¤‰æ›´ï¼‰

```python
policy_loss_weight = 0.0  # â˜… Policy lossã‚’å®Œå…¨é™¤å¤–
value_loss_weight = 1.0   # Value lossã®ã¿ã§è¨“ç·´
```

**ç†è«–çš„èƒŒæ™¯**:
- Wang & Emmerich (2019)ãŒ6x6 Othello/Connect Fourã§å®Ÿè¨¼
- **Value-only lossãŒæœ€é«˜tournament Eloé”æˆ**
- AlphaZeroãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼ˆpolicy + valueï¼‰ã‚ˆã‚Šä¸€è²«ã—ã¦å„ªä½
- Blokus Duo (14x14)ã‚‚ã€Œå°ã•ã„ã‚²ãƒ¼ãƒ ã€ã‚«ãƒ†ã‚´ãƒªã«è©²å½“

### ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | å€¤ | å‚™è€ƒ |
|-----------|-----|------|
| Iterations | 40 | Iter 40ãŒæ—§ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§æœ€é«˜æ€§èƒ½ |
| Games/iter | 5 | æ¨™æº–è¨­å®š |
| MCTS sims | 200 | Iter 40ã®æˆåŠŸè¨­å®š |
| Eval interval | 5 | 5ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã”ã¨ |
| Eval games | 20 | çµ±è¨ˆçš„ä¿¡é ¼æ€§å‘ä¸Š |
| Learning rate | 5e-4 | Iter 40ã®æˆåŠŸè¨­å®š |
| Replay buffer | ç„¡åŠ¹ | Iter 40ã®è¨­å®š |
| Batch size | 32 | æ¨™æº–è¨­å®š |

### ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

- **æ–°ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**: GroupNorm + 64ch value head (375K params)
- BatchNorm â†’ GroupNormï¼ˆå°ãƒãƒƒãƒå­¦ç¿’å®‰å®šåŒ–ï¼‰
- Value headæ‹¡å¼µï¼ˆ32â†’64ch, æ·±ã„MLP, Dropoutè¿½åŠ ï¼‰

---

## å®Ÿè¡Œã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹

### ãƒ—ãƒ­ã‚»ã‚¹æƒ…å ±

- **PID**: 19267
- **CPUä½¿ç”¨ç‡**: 735% (7åˆ†çµŒé)
- **ãƒ¡ãƒ¢ãƒª**: 2.5% (402MB)
- **çŠ¶æ…‹**: å®Ÿè¡Œä¸­ ğŸŸ¢

### WandB

- **Project**: BlokusAI-ValueOnly
- **Run**: northern-dust-2
- **URL**: https://wandb.ai/onukura-personal/BlokusAI-ValueOnly/runs/q5mx91x9

### ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°

```bash
# ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–
tail -f /tmp/claude/-home-ubuntu-dev-personal-BlokusAI/tasks/b4988cf.output

# ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ç¢ºèª
/tmp/monitor_value_only.sh
```

---

## æœŸå¾…ã•ã‚Œã‚‹çµæœ

### ä¿å®ˆçš„äºˆæ¸¬

| æŒ‡æ¨™ | ç¾çŠ¶ï¼ˆIter 40æ—§ï¼‰ | æœŸå¾… | æ”¹å–„ |
|------|------------------|------|------|
| Valueç›¸é–¢ | 0.63 | **0.70+** | +10% |
| AI vs Random | 25% | **50%** | +100% |
| AI vs Greedy | 0% | **15-30%** | +âˆ |

**æ ¹æ‹ **:
- Value headã ã‘ã§MCTSãŒæ©Ÿèƒ½ã™ã‚Œã°Greedyä»¥ä¸Šã®æ€§èƒ½
- Policy headã®èª¤å­¦ç¿’ãŒãªã„ãŸã‚å®‰å®šã—ãŸæ”¹å–„

### æ¥½è¦³çš„äºˆæ¸¬

| æŒ‡æ¨™ | æœŸå¾… |
|------|------|
| Valueç›¸é–¢ | **0.80+** |
| AI vs Random | **60-70%** |
| AI vs Greedy | **40-50%** |

**æ ¹æ‹ **:
- Wang & Emmerichã§Value-onlyãŒ**æœ€é«˜Elo**
- Policy headã®å¹²æ¸‰ãªã—
- æ–°ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®æ©æµ

---

## æˆåŠŸåŸºæº–

### æœ€ä½åŸºæº–ï¼ˆå®Ÿé¨“æˆåŠŸï¼‰

- âœ… Valueç›¸é–¢ > 0.6ï¼ˆç¶­æŒï¼‰
- âœ… AI vs Random > 40%ï¼ˆç¾åœ¨25%ã‹ã‚‰æ”¹å–„ï¼‰
- âœ… AI vs Greedy > 10%ï¼ˆç¾åœ¨0%ã‹ã‚‰æ”¹å–„ï¼‰

### ç›®æ¨™åŸºæº–ï¼ˆå¼·ã„æˆåŠŸï¼‰

- âœ… Valueç›¸é–¢ > 0.7
- âœ… AI vs Random > 50%ï¼ˆæœŸå¾…å€¤ï¼‰
- âœ… AI vs Greedy > 30%

### ç†æƒ³åŸºæº–ï¼ˆå®Œå…¨æˆåŠŸï¼‰

- âœ… Valueç›¸é–¢ > 0.8
- âœ… AI vs Random > 60%
- âœ… AI vs Greedy > 50%ï¼ˆGreedyã‚’è¶…ãˆã‚‹ï¼‰

---

## æ—¢çŸ¥ã®ãƒªã‚¹ã‚¯

### ãƒªã‚¹ã‚¯1: Policy HeadãŒå…¨ãå­¦ç¿’ã—ãªã„

**å¯¾ç­–**: è«–æ–‡ã«ã‚ˆã‚Œã°å•é¡Œãªã„
- Policy headã¯MCTS priorã¨ã—ã¦ã®å½¹å‰²ã®ã¿
- Uniform priorã§ã‚‚æ€§èƒ½ä½ä¸‹ã¯å°ã•ã„
- Value headã®æ­£ç¢ºæ€§ãŒã‚ˆã‚Šé‡è¦

### ãƒªã‚¹ã‚¯2: æ–°ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®ç›¸æ€§

**å¯¾ç­–**: ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¯æ”¹å–„ç‰ˆ
- GroupNorm: å°ãƒãƒƒãƒã§ã‚‚å®‰å®š
- æ·±ã„Value head: ã‚ˆã‚Šæ­£ç¢ºãªæ¨å®š
- Dropout: éå­¦ç¿’é˜²æ­¢

### ãƒªã‚¹ã‚¯3: éå­¦ç¿’

**å¯¾ç­–**:
- Eval interval 5ã§é »ç¹ã«è©•ä¾¡
- Past generations checkpointã§ç›£è¦–
- å¿…è¦ãªã‚‰iter 30-40ã§æ—©æœŸåœæ­¢

---

## æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

### å®Ÿé¨“æˆåŠŸæ™‚

1. **è©³ç´°åˆ†æ**
   - Valueç›¸é–¢ã®æ¨ç§»
   - Policy distributionã®å¤‰åŒ–
   - MCTS visit distributionã®è³ª

2. **é•·æœŸãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°**
   - 60-100ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã¾ã§å»¶é•·
   - Early stoppingå®Ÿè£…
   - Replay bufferæœ‰åŠ¹åŒ–ãƒ†ã‚¹ãƒˆ

3. **Pentobiè©•ä¾¡**
   - ãƒ¬ãƒ™ãƒ«3, 5, 7ã¨ã®å¯¾æˆ¦

### å®Ÿé¨“å¤±æ•—æ™‚

1. **Phase 2: Policy Scaling**
   - `policy_loss_weight = 1.5` (KataGo style)

2. **Phase 3: Hybrid**
   - `policy_loss_weight = 0.01` + Entropy regularization

3. **Phase 4: Imitation Learning**
   - Greedyæˆ¦ç•¥ã®äº‹å‰å­¦ç¿’

---

## ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ï¼ˆæ¨å®šï¼‰

| ãƒ•ã‚§ãƒ¼ã‚º | æ™‚é–“ | å‚™è€ƒ |
|---------|------|------|
| Iter 1-5 | ~30-45åˆ† | åˆæœŸå­¦ç¿’ã€æœ€åˆã®è©•ä¾¡ |
| Iter 6-10 | ~30-45åˆ† | 2å›ç›®ã®è©•ä¾¡ |
| Iter 11-15 | ~30-45åˆ† | 3å›ç›®ã®è©•ä¾¡ |
| Iter 16-20 | ~30-45åˆ† | 4å›ç›®ã®è©•ä¾¡ |
| Iter 21-40 | ~2-3æ™‚é–“ | å¾ŒåŠ |
| **Total** | **~4-6æ™‚é–“** | 40ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Œäº† |

---

## å‚è€ƒæ–‡çŒ®

1. **Wang & Emmerich (2019)**: "Policy or Value? Loss Function and Playing Strength in AlphaZero-like Self-play"
   - https://liacs.leidenuniv.nl/~plaata1/papers/CoG2019.pdf

2. **Wu (2019)**: "Accelerating Self-Play Learning in Go" (KataGo)
   - https://arxiv.org/pdf/1902.10565

3. **Silver et al. (2017)**: "Mastering Chess and Shogi by Self-Play" (AlphaZero)
   - https://www.science.org/doi/10.1126/science.aar6404

---

**Status**: ğŸŸ¢ å®Ÿè¡Œä¸­
**æœ€çµ‚æ›´æ–°**: 2026-01-20 23:52 JST
