# Training Diagnosis Report
**Date**: 2026-01-18
**Training Run**: train_fixed.py (20 iterations, 500 MCTS simulations)

## Executive Summary

ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¯20ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Œäº†ã—ãŸãŒã€**å­¦ç¿’ã¯å¤±æ•—**ã—ã¦ã„ã‚‹ã€‚Greedyãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã«å¯¾ã—ã¦å…¨ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§0%å‹ç‡ã‚’è¨˜éŒ²ã—ã€ãƒ¢ãƒ‡ãƒ«ã¯å®Ÿç”¨çš„ãªæˆ¦ç•¥ã‚’ç²å¾—ã§ãã¦ã„ãªã„ã€‚

## Training Configuration

| Parameter | Value |
|-----------|-------|
| Iterations | 20 |
| Games per iteration | 5 |
| MCTS simulations | 500 |
| Learning rate | 5e-4 |
| LR scheduler | Disabled |
| Replay buffer | Enabled (10000 max, 1000 min) |
| Training steps/iter | 100 |

## Performance Results

### Win Rates vs Baselines

| Iteration | vs Random | vs Greedy | vs Past (iter-5) | vs Past (iter-10) |
|-----------|-----------|-----------|------------------|-------------------|
| 5  | 40% | **0%** | - | - |
| 10 | 60% | **0%** | 100% âœ… | - |
| 15 | 40% | **0%** | 0% âŒ | 100% âœ… |
| 20 | 40% | **0%** | 100% âœ… | 0% âŒ |

### Critical Issues

#### 1. **Intransitive Performance Ordering** ğŸ”´

éå»ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒã§æ¨ç§»çš„é–¢ä¿‚ãŒæˆç«‹ã—ã¦ã„ãªã„ï¼š

```
Iter 20 > Iter 15 (100%)
Iter 15 > Iter 10 (100%)
BUT Iter 20 < Iter 10 (0%)
```

**å«æ„**: ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ãŒå¾ªç’°çš„ã«å¤‰å‹•ã—ã¦ãŠã‚Šã€ä¸€è²«ã—ãŸæ”¹å–„ãŒãªã„ã€‚

#### 2. **Complete Failure vs Greedy** ğŸ”´

å…¨ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§Greedyã«0%å‹ç‡ï¼š
- Greedyã¯å˜ç´”ã«ã€Œæœ€å¤§ã‚µã‚¤ã‚ºã®ãƒ”ãƒ¼ã‚¹ã‹ã‚‰ç½®ãã€æˆ¦ç•¥
- ã“ã‚Œã¯æœ€ã‚‚åŸºæœ¬çš„ãªãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯
- 500 MCTSã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§ã‚‚å‹ã¦ãªã„

**å«æ„**: ãƒ¢ãƒ‡ãƒ«ã¯åŸºæœ¬çš„ãªæˆ¦ç•¥ã™ã‚‰å­¦ç¿’ã—ã¦ã„ãªã„ã€‚

#### 3. **Unstable Performance vs Random**

Randomã«å¯¾ã™ã‚‹å‹ç‡ãŒä¸å®‰å®šï¼š40% â†’ 60% â†’ 40% â†’ 40%

**å«æ„**: è¨“ç·´ãŒåæŸã—ã¦ã„ãªã„ã€ã¾ãŸã¯éå­¦ç¿’ã¨å¿˜å´ã‚’ç¹°ã‚Šè¿”ã—ã¦ã„ã‚‹ã€‚

## Detailed Model Analysis

### Checkpoint Comparison (Initial Position)

#### Value Head Evolution

| Iteration | Value Output | Interpretation |
|-----------|--------------|----------------|
| 5  | -0.17 | ã‚„ã‚„æ‚²è¦³çš„ |
| 10 | -0.19 | ã‚„ã‚„æ‚²è¦³çš„ |
| 15 | **-0.79** | éå¸¸ã«æ‚²è¦³çš„ |
| 20 | **-1.00** | ã»ã¼æœ€æ‚ªå€¤ï¼ˆç¢ºå®Ÿã«è² ã‘ã‚‹ã¨äºˆæ¸¬ï¼‰ |

**ç•°å¸¸**: åˆæœŸå±€é¢ã¯å¯¾ç§°ãªã®ã§æœŸå¾…å€¤ã¯0ä»˜è¿‘ã§ã‚ã‚‹ã¹ãã€‚Value headãŒè¨“ç·´ã‚’é€šã˜ã¦ã©ã‚“ã©ã‚“æ‚²è¦³çš„ã«ãªã£ã¦ã„ã‚‹ã€‚

#### Policy Head Metrics

| Iteration | Max Prob | Entropy | Greedy Rank | Greedy Prob | Top Move Size |
|-----------|----------|---------|-------------|-------------|---------------|
| 5  | 2.0% | 4.05 | 55/58 | 1.5% | 4 |
| 10 | **11.2%** | 3.72 | 56/58 | 0.5% | 5 |
| 15 | 8.2% | 3.72 | 50/58 | 0.4% | 2 |
| 20 | 6.2% | 3.84 | 53/58 | 0.7% | **5** |

**è¦³å¯Ÿ**:
- Iteration 5: ã»ã¼ãƒ•ãƒ©ãƒƒãƒˆãªåˆ†å¸ƒï¼ˆæœªå­¦ç¿’çŠ¶æ…‹ï¼‰
- Iteration 10: é›†ä¸­ã—å§‹ã‚ãŸï¼ˆmax prob 11%ï¼‰ã€ã‚µã‚¤ã‚º5ã‚’é¸æŠ
- Iteration 15: **é€€åŒ–**ï¼ˆã‚µã‚¤ã‚º2ã‚’ãƒˆãƒƒãƒ—ã«é¸æŠï¼‰
- Iteration 20: éƒ¨åˆ†çš„å›å¾©ï¼ˆã‚µã‚¤ã‚º5ãŒãƒˆãƒƒãƒ—ã€60%ï¼‰

**å•é¡Œ**:
1. **Greedyæ‰‹ã®é †ä½**: ã™ã¹ã¦ã®ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§æœ€ä¸‹ä½ä»˜è¿‘ï¼ˆ50-56ä½/58æ‰‹ï¼‰
2. **ç¢ºç‡ã®ä½ã•**: Greedyæ‰‹ã®ç¢ºç‡ãŒ0.4-1.5%ã®ã¿
3. **ä¸å®‰å®šæ€§**: æ”¹å–„ã¨é€€åŒ–ã‚’ç¹°ã‚Šè¿”ã™

#### Top 10 Moves Size Distribution

| Iteration | Size 5 | Size 4 | Size 3 | Size 2 | Size 1 |
|-----------|--------|--------|--------|--------|--------|
| 5  | 0% | **60%** | 10% | 20% | 10% |
| 10 | **30%** | 50% | 0% | 20% | 0% |
| 15 | **60%** | 20% | 0% | 20% | 0% |
| 20 | **60%** | 20% | 0% | 20% | 0% |

**è¦³å¯Ÿ**: Iteration 10ä»¥é™ã¯ã‚µã‚¤ã‚º5ã®ãƒ”ãƒ¼ã‚¹ãŒå¢—åŠ å‚¾å‘ã ãŒã€Greedyæˆ¦ç•¥ï¼ˆã‚µã‚¤ã‚º5ã‚’æœ€å„ªå…ˆï¼‰ã«ã¯å±Šã„ã¦ã„ãªã„ã€‚

## Training Loss Progression

| Iteration | Total Loss | Policy Loss | Value Loss |
|-----------|------------|-------------|------------|
| 7  | 4.48 | 4.12 | 0.36 |
| 10 | 3.86 | 3.78 | 0.09 |
| 15 | 3.81 | 3.66 | 0.15 |
| 20 | 3.61 | 3.49 | 0.12 |

**è¦³å¯Ÿ**:
- Total loss: 4.48 â†’ 3.61 (19%æ¸›å°‘)
- Policy loss: 4.12 â†’ 3.49 (15%æ¸›å°‘)
- Value loss: 0.36 â†’ 0.12 (67%æ¸›å°‘)

**çŸ›ç›¾**: Loss ã¯æ¸›å°‘ã—ã¦ã„ã‚‹ãŒã€å®Ÿéš›ã®å¯¾æˆ¦æ€§èƒ½ã¯æ”¹å–„ã—ã¦ã„ãªã„ã€‚

â†’ ãƒ¢ãƒ‡ãƒ«ãŒè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«**éå­¦ç¿’**ã—ã¦ã„ã‚‹ãŒã€**æ±åŒ–**ã—ã¦ã„ãªã„å¯èƒ½æ€§ã€‚

## Training Data Analysis Results âœ…

### Self-play Game Statistics

| Iteration | P0 Win Rate | Avg Length | Value Target Mean | Value Pred Mean | Correlation | MSE |
|-----------|-------------|------------|-------------------|-----------------|-------------|-----|
| 5  | 60% | 27.9 | **+0.068** | -0.026 | **0.147** | 0.986 |
| 10 | 30% | 29.0 | **+0.062** | -0.348 | **0.104** | 1.175 |
| 15 | 40% | 27.2 | **+0.088** | -0.208 | **-0.010** | 1.467 |
| 20 | 40% | 27.0 | **+0.096** | -0.099 | **0.019** | 1.471 |

### Critical Findings

#### 1. Value Target Distribution is Normal âœ…

- Value targetå¹³å‡: +0.06 ~ +0.10ï¼ˆå¯¾ç§°çš„ã€æ­£å¸¸ï¼‰
- Win/Lossåˆ†å¸ƒ: ç´„50/50ï¼ˆæ­£å¸¸ï¼‰
- Draws: 0-10%ï¼ˆæ­£å¸¸ï¼‰

â†’ **Value targetã®è¨ˆç®—ã«ã¯å•é¡Œãªã—**

#### 2. Value Head Completely Fails to Learn ğŸ”´

**Evidence**:

| Iteration | Target Mean | Pred Mean | **Gap** | Pred by Target (+1) | Pred by Target (-1) |
|-----------|-------------|-----------|---------|---------------------|---------------------|
| 5  | +0.068 | -0.026 | **-0.094** | -0.014 | -0.040 |
| 10 | +0.062 | -0.348 | **-0.410** | -0.310 | -0.409 |
| 15 | +0.088 | -0.208 | **-0.296** | -0.214 | -0.202 |
| 20 | +0.096 | -0.099 | **-0.195** | -0.087 | -0.113 |

**å‹ã¡ã‚²ãƒ¼ãƒ (target=+1)ã§ã‚‚è² ã®å€¤ã‚’äºˆæ¸¬**:
- Iter 5: -0.014ï¼ˆã¾ã ãƒã‚·ï¼‰
- Iter 10: **-0.310**ï¼ˆæ‚²è¦³çš„ï¼‰
- Iter 15: **-0.214**ï¼ˆæ‚²è¦³çš„ï¼‰
- Iter 20: -0.087ï¼ˆã‚„ã‚„æ”¹å–„ï¼‰

#### 3. Correlation Collapses to Zero ğŸ”´

| Iteration | Correlation | Interpretation |
|-----------|-------------|----------------|
| 5  | 0.147 | å¼±ã„æ­£ã®ç›¸é–¢ï¼ˆå­¦ç¿’ã®å…†ã—ï¼‰ |
| 10 | 0.104 | ã•ã‚‰ã«å¼±ã |
| 15 | **-0.010** | **ã»ã¼ç„¡ç›¸é–¢ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ï¼‰** |
| 20 | **0.019** | **ã»ã¼ç„¡ç›¸é–¢ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ï¼‰** |

â†’ Value headã¯**ãƒ©ãƒ³ãƒ€ãƒ äºˆæ¸¬ã¨åŒç­‰**

#### 4. MSE Increases with Training ğŸ”´

| Iteration | MSE | Change |
|-----------|-----|--------|
| 5  | 0.986 | baseline |
| 10 | 1.175 | +19% |
| 15 | 1.467 | +49% |
| 20 | 1.471 | +49% |

â†’ è¨“ç·´ã‚’ç¶šã‘ã‚‹ã»ã©**æ€§èƒ½ãŒæ‚ªåŒ–**

## Root Cause: VALUE HEAD TRAINING FAILURE

### Confirmed Root Cause

**Value headãŒå®Œå…¨ã«å­¦ç¿’ã‚’å¤±æ•—ã—ã¦ã„ã‚‹**

1. **Correlation â‰ˆ 0**: äºˆæ¸¬ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã«ç›¸é–¢ãªã—ï¼ˆãƒ©ãƒ³ãƒ€ãƒ äºˆæ¸¬ï¼‰
2. **MSEå¢—åŠ **: è¨“ç·´ã§æ€§èƒ½ãŒæ‚ªåŒ–
3. **è² ã®ãƒã‚¤ã‚¢ã‚¹**: æ­£ã—ã„å€¤(+0.1)ã§ã¯ãªãè² ã®å€¤(-0.1)ã‚’å‡ºåŠ›
4. **å‹ã¡å±€é¢ã§ã‚‚æ‚²è¦³çš„**: target=+1ã§ã‚‚pred=-0.3ã‚’å‡ºåŠ›

### Why Value Head Failure Causes Everything Else to Fail

Value headãŒå£Šã‚Œã‚‹ã¨ï¼š
1. **MCTSã®æ¢ç´¢ãŒæ­ªã‚€** â†’ æ‚²è¦³çš„ãªè©•ä¾¡ã§æ¢ç´¢ã‚’èª¤ã‚‹
2. **Policy headã®å­¦ç¿’ãŒå¦¨ã’ã‚‰ã‚Œã‚‹** â†’ èª¤ã£ãŸvalue guidanceã§èª¤ã£ãŸæ–¹å‘ã«å­¦ç¿’
3. **è‡ªå·±å¯¾æˆ¦ã®è³ªãŒä½ä¸‹** â†’ å¼±ã„å¯¾æˆ¦ç›¸æ‰‹ã¨å¯¾æˆ¦ã—ã¦ã‚‚ã‚¹ã‚­ãƒ«ãŒå‘ä¸Šã—ãªã„

â†’ ã“ã‚ŒãŒ**å…¨ä½“çš„ãªå­¦ç¿’å¤±æ•—ã®æ ¹æœ¬åŸå› **

### 2. Policy Head Not Learning Strategic Concepts

**è¨¼æ‹ **:
- Greedyæ‰‹ï¼ˆã‚µã‚¤ã‚º5å„ªå…ˆï¼‰ãŒæœ€ä¸‹ä½ä»˜è¿‘
- Top moveãŒã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³é–“ã§ä¸å®‰å®šï¼ˆsize 5 â†’ 2 â†’ 5ï¼‰

**ä»®èª¬**:
- Policy headã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ãŒä¸é©åˆ‡ï¼Ÿ
- MCTSã®æ¢ç´¢ãŒåŠ¹æœçš„ã§ãªã„ï¼Ÿ
- Learning rateãŒé«˜ã™ãã¦å®‰å®šã—ãªã„ï¼Ÿ

### 3. MCTS Evaluation Instability

**è¨¼æ‹ **:
- æ¨ç§»çš„é–¢ä¿‚ã®å´©å£Šï¼ˆIter 20 > 15 > 10 ã ãŒ 20 < 10ï¼‰
- åŒã˜ãƒ¢ãƒ‡ãƒ«ã§ã‚‚è©•ä¾¡ã®ãŸã³ã«çµæœãŒå¤‰ã‚ã‚‹å¯èƒ½æ€§

**ä»®èª¬**:
- MCTS 500ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§ã‚‚ãƒã‚¤ã‚ºãŒå¤§ãã„ï¼Ÿ
- è©•ä¾¡ã‚²ãƒ¼ãƒ æ•°ï¼ˆ10ã‚²ãƒ¼ãƒ ï¼‰ãŒå°‘ãªã™ãã‚‹ï¼Ÿ
- æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å•é¡Œï¼Ÿ

### 4. Replay Buffer Side Effects

**è¨¼æ‹ **:
- Iteration 7ã‹ã‚‰è¨“ç·´é–‹å§‹ï¼ˆbuffer >= 1000ï¼‰
- ãã®å¾Œã®æ€§èƒ½ãŒä¸å®‰å®š

**ä»®èª¬**:
- å¤ã„ãƒ‡ãƒ¼ã‚¿ã¨æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã®æ··åœ¨ãŒå­¦ç¿’ã‚’å¦¨ã’ã‚‹ï¼Ÿ
- Buffer sizeãŒå¤§ãã™ãã‚‹ï¼ˆ10000ï¼‰ï¼Ÿ
- ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®åã‚Šï¼Ÿ

## Recommended Fix Strategy

### Phase 1: Immediate Fixes (High Priority) ğŸ”´

#### 1. Drastically Reduce Value Loss Weight âš ï¸ **CRITICAL**

**Current**: value_loss_weight = 1.0 (equal to policy loss)
**Problem**: Value headãŒéå­¦ç¿’ã—ã€ç›¸é–¢ãŒã‚¼ãƒ­ã«
**Fix**: value_loss_weight = **0.01** (100å€å‰Šæ¸›)

**Rationale**:
- Value lossãŒå¤§ãã™ãã¦policy lossã‚’æ”¯é…
- Policyå­¦ç¿’ã‚’å„ªå…ˆã—ã€valueã¯è£œåŠ©çš„ã«
- AlphaZeroè«–æ–‡ã§ã‚‚ value weight < policy weight

#### 2. Disable or Reduce Replay Buffer âš ï¸ **CRITICAL**

**Current**: buffer_size=10000, min_buffer_size=1000
**Problem**: å¤ã„ãƒ‡ãƒ¼ã‚¿ã¨æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã®æ··åœ¨
**Fix Option A**: buffer_size = 0ï¼ˆç„¡åŠ¹åŒ–ï¼‰
**Fix Option B**: buffer_size = 500, min_buffer_size = 100

**Rationale**:
- è‡ªå·±æ”¹å–„å‹å­¦ç¿’ã§ã¯æœ€æ–°ãƒ‡ãƒ¼ã‚¿ãŒæœ€é‡è¦
- å¤ã„ãƒ‡ãƒ¼ã‚¿ã¯ç¾åœ¨ã®ãƒãƒªã‚·ãƒ¼ã¨çŸ›ç›¾

#### 3. Reduce Learning Rate

**Current**: 5e-4
**Fix**: **1e-4** ã¾ãŸã¯ **5e-5**

**Rationale**: ã‚ˆã‚Šå®‰å®šã—ãŸå­¦ç¿’

#### 4. Reduce MCTS Simulations

**Current**: 500
**Fix**: **100**

**Rationale**:
- è¨ˆç®—ã‚³ã‚¹ãƒˆå‰Šæ¸›
- éåº¦ãªæ¢ç´¢ãŒãƒã‚¤ã‚ºã‚’ç”Ÿã‚€å¯èƒ½æ€§

### Phase 2: Training Configuration Changes

**Recommended minimal config for testing**:

```python
main(
    num_iterations=20,
    games_per_iteration=5,
    num_simulations=100,          # 500 â†’ 100
    eval_interval=5,
    eval_games=20,                # 10 â†’ 20ï¼ˆè©•ä¾¡ã®ä¿¡é ¼æ€§å‘ä¸Šï¼‰
    past_generations=[5],         # ã‚·ãƒ³ãƒ—ãƒ«ã«
    use_wandb=True,
    buffer_size=0,                # â˜… DISABLED
    batch_size=32,
    num_training_steps=50,        # 100 â†’ 50ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ã«ï¼‰
    min_buffer_size=0,
    learning_rate=1e-4,           # 5e-4 â†’ 1e-4
    value_loss_weight=0.01,       # â˜… NEW: 1.0 â†’ 0.01
    max_grad_norm=1.0,
    use_lr_scheduler=False,
    mcts_batch_size=16,
    num_workers=1,
)
```

### Phase 3: Architecture Changes (If Phase 1-2 Fails)

#### Option A: Separate Optimizers

- Policy headã¨value headã«åˆ¥ã€…ã®optimizer
- Value headã ã‘lower learning rate

#### Option B: Simpler Value Head

- MLPã®å±¤æ•°ã‚’å‰Šæ¸›
- éå­¦ç¿’ã‚’é˜²ããŸã‚ã®dropoutè¿½åŠ 

#### Option C: Value Clipping

- Value targetã‚’[-0.9, 0.9]ã«ã‚¯ãƒªãƒƒãƒ—
- æ¥µç«¯ãªå€¤ã‚’é˜²ã

### Phase 4: Alternative Approaches (If All Fails)

#### Option A: Policy-Only Training

- Value headã‚’å®Œå…¨ã«ç„¡åŠ¹åŒ–
- MCTSã®ã¿ã§valueæ¨å®šï¼ˆrolloutï¼‰
- ã¾ãšpolicyã‚’å­¦ç¿’ã•ã›ã‚‹

#### Option B: Supervised Pre-training

- Greedyãƒãƒªã‚·ãƒ¼ã‚’imitationLearning
- åŸºæœ¬æˆ¦ç•¥ã‚’å…ˆã«å­¦ç¿’
- ãã®å¾Œself-playã§fine-tune

#### Option C: Curriculum Learning

- å°ã•ã„ãƒœãƒ¼ãƒ‰ï¼ˆ10x10ï¼‰ã‹ã‚‰é–‹å§‹
- ç°¡å˜ãªå•é¡Œã§åŸºç¤ã‚’å­¦ç¿’
- å¾ã€…ã«é›£æ˜“åº¦ã‚’ä¸Šã’ã‚‹

## Conclusion

### Diagnosis Complete âœ…

ç¾åœ¨ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¯**æŠ€è¡“çš„ã«ã¯å®Œäº†ã—ãŸãŒã€å­¦ç¿’ã¯å®Œå…¨ã«å¤±æ•—**ã—ã¦ã„ã‚‹ã€‚

### Root Cause Confirmed ğŸ”

**Value Head Training Failure** ãŒå…¨ã¦ã®å•é¡Œã®æ ¹æœ¬åŸå› ï¼š

1. âœ… **Value targetã¯æ­£å¸¸**ï¼ˆå¹³å‡+0.07ã€åˆ†å¸ƒ50/50ï¼‰
2. âŒ **Value predictionãŒç ´ç¶»**:
   - Correlation: 0.15 â†’ **0.02**ï¼ˆã»ã¼ã‚¼ãƒ­ï¼‰
   - MSE: 0.99 â†’ **1.47**ï¼ˆ+49%æ‚ªåŒ–ï¼‰
   - å‹ã¡å±€é¢(+1)ã§ã‚‚è² ã®å€¤(-0.3)ã‚’äºˆæ¸¬
3. âŒ **è¨“ç·´ã§æ€§èƒ½ãŒæ‚ªåŒ–**: Iter 5ãŒæœ€è‰¯ã€ãã®å¾Œå´©å£Š

### Cascade Effect

Value headã®å¤±æ•— â†’ MCTSæ¢ç´¢ã®æ­ªã¿ â†’ Policyå­¦ç¿’ã®å¤±æ•— â†’ å…¨ä½“çš„ãªæ€§èƒ½ä½ä¸‹

### Next Action: Fix Attempt ğŸ”§

**æœ€å„ªå…ˆäº‹é …ï¼ˆPhase 1ï¼‰**:
1. **Value loss weight: 1.0 â†’ 0.01** âš ï¸ CRITICAL
2. **Replay buffer: ç„¡åŠ¹åŒ–** âš ï¸ CRITICAL
3. **Learning rate: 5e-4 â†’ 1e-4**
4. **MCTS sims: 500 â†’ 100**

**Expected outcome**:
- Value headãŒéå­¦ç¿’ã›ãšã€ç›¸é–¢ãŒç¶­æŒã•ã‚Œã‚‹
- Policy headãŒã‚ˆã‚Šå®‰å®šã—ã¦å­¦ç¿’ã™ã‚‹
- Greedyæˆ¦ç•¥ã‚’ç²å¾—ã§ãã‚‹

**Success criteria**:
- Value correlation > 0.3ï¼ˆIter 5ã®2å€ï¼‰
- AI vs Greedy > 50%
- æ¨ç§»çš„é–¢ä¿‚ãŒæˆç«‹

---

**Status**: âœ… Diagnosis complete. Ready for fix implementation.
