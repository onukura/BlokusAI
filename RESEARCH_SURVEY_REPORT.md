# AlphaZeroç³»ãƒœãƒ¼ãƒ‰ã‚²ãƒ¼ãƒ AI èª¿æŸ»å ±å‘Šæ›¸

**æ—¥ä»˜**: 2026-01-20
**ç›®çš„**: BlokusAIè¨“ç·´ã®å•é¡Œï¼ˆPolicy headæœªå­¦ç¿’ã€Greedyæˆ¦ç•¥ã«0%å‹ç‡ï¼‰ã«å¯¾ã™ã‚‹è§£æ±ºç­–ã‚’æ–‡çŒ®ãƒ»å®Ÿè£…äº‹ä¾‹ã‹ã‚‰æ¢ã‚‹
**èª¿æŸ»ç¯„å›²**: Blokus AIå®Ÿè£…ã€AlphaZeroãƒœãƒ¼ãƒ‰ã‚²ãƒ¼ãƒ AIã€æå¤±é–¢æ•°è¨­è¨ˆã€è¨“ç·´æ‰‹æ³•

---

## ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼

### ğŸ¯ æœ€é‡è¦ç™ºè¦‹

**Wang & Emmerich (2019)ã®ç ”ç©¶**ã«ã‚ˆã‚Šã€**å°ã•ã„ã‚²ãƒ¼ãƒ ã§ã¯value lossã®ã¿ã§è¨“ç·´ã™ã‚‹æ–¹ãŒæ€§èƒ½ãŒé«˜ã„**ã“ã¨ãŒå®Ÿè¨¼ã•ã‚Œã¦ã„ã¾ã™ã€‚

- 6x6 Othelloã€Connect Fourã§å®Ÿé¨“
- **Value-only loss**ãŒround-robin tournamentã§**æœ€é«˜Eloé”æˆ**
- AlphaZeroã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼ˆpolicy + valueï¼‰ã‚ˆã‚Šä¸€è²«ã—ã¦å„ªä½
- Blokus Duo (14x14)ã‚‚æ¯”è¼ƒçš„å°ã•ã„ã‚²ãƒ¼ãƒ ãªã®ã§åŒæ§˜ã®å‚¾å‘ãŒæœŸå¾…ã•ã‚Œã‚‹

### æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³

**å³åº§ã«value-only trainingï¼ˆpolicy_loss_weight=0.0ï¼‰ã‚’è©¦ã™**

ç†ç”±ï¼š
1. æˆ‘ã€…ã®å•é¡Œï¼ˆValueç›¸é–¢0.63ã§ã‚‚å®Ÿæˆ¦0%ï¼‰ã«å®Œå…¨ã«åˆè‡´
2. æŸ»èª­æ¸ˆã¿è«–æ–‡ã§å®Ÿè¨¼ã•ã‚ŒãŸæ‰‹æ³•
3. å®Ÿè£…ãŒç°¡å˜ï¼ˆ1è¡Œã®å¤‰æ›´ï¼‰
4. ãƒªã‚¹ã‚¯ãŒä½ã„

---

## è©³ç´°èª¿æŸ»çµæœ

### 1. BlokusAIå®Ÿè£…äº‹ä¾‹

#### 1.1 GitHubå®Ÿè£…

| ãƒªãƒã‚¸ãƒˆãƒª | ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  | ç‰¹å¾´ | æ€§èƒ½å ±å‘Š |
|-----------|------------|------|---------|
| [KubiakJakub01/Blokus-RL](https://github.com/KubiakJakub01/Blokus-RL) | PPO, AlphaZero | 7x7, 20x20å¯¾å¿œã€PyTorch | âŒ ãªã— |
| [roger-creus/blokus-ai](https://github.com/roger-creus/blokus-ai) | - | Gymnasiumç’°å¢ƒ | âŒ ãªã— |
| [ytolochko/AlphaZero](https://github.com/ytolochko/AlphaZero) | AlphaZero | Blokuså°‚ç”¨å®Ÿè£… | âŒ ãªã— |
| [DerekGloudemans/Blokus-RL](https://github.com/DerekGloudemans/Blokus-Reinforcement-Learning) | Heuristics, RL | åŠ¹ç‡çš„å®Ÿè£…ç›®æ¨™ | âŒ ãªã— |

**è¦³å¯Ÿ**:
- è¤‡æ•°ã®å®Ÿè£…ãŒå­˜åœ¨ã™ã‚‹ãŒã€**æ€§èƒ½çµæœãŒå…¬é–‹ã•ã‚Œã¦ã„ã‚‹ã‚‚ã®ã¯ã‚¼ãƒ­**
- Blokusã¯**è¨“ç·´ãŒé›£ã—ã„ã‚²ãƒ¼ãƒ **ã¨æ¨æ¸¬ã•ã‚Œã‚‹
- DeepMindå…¬å¼ã®AlphaZeroã‚‚Blokusã¯å¯¾è±¡å¤–ï¼ˆChess, Shogi, Goã®ã¿ï¼‰

#### 1.2 å«æ„

**æˆ‘ã€…ã¯å…ˆè¡Œç ”ç©¶ã®ãªã„ãƒ•ãƒ­ãƒ³ãƒ†ã‚£ã‚¢ã«ã„ã‚‹**
- Blokusã§ã®æˆåŠŸäº‹ä¾‹ãŒãªã„
- ä»–ã®å®Ÿè£…ã‚‚åŒæ§˜ã®å•é¡Œã«ç›´é¢ã—ã¦ã„ã‚‹å¯èƒ½æ€§
- ã ã‹ã‚‰ã“ãæ–‡çŒ®ã®ç†è«–çš„çŸ¥è¦‹ãŒé‡è¦

---

### 2. æå¤±é–¢æ•°è¨­è¨ˆã«é–¢ã™ã‚‹é‡è¦ç ”ç©¶

#### 2.1 Wang & Emmerich (2019): "Policy or Value?"

**è«–æ–‡**: ["Policy or Value? Loss Function and Playing Strength in AlphaZero-like Self-play"](https://www.semanticscholar.org/paper/Policy-or-Value-Loss-Function-and-Playing-Strength-Wang-Emmerich/b125c8933d0264b9a103cb8fa80f226f8c9c3cdc)

**å®Ÿé¨“è¨­å®š**:
- ã‚²ãƒ¼ãƒ : 5x5/6x6 Othello, 5x5/6x6 Connect Four
- å®Ÿè£…: AlphaZeroGeneral
- æå¤±é–¢æ•°ã®æ¯”è¼ƒ:
  1. `loss_pi`ï¼ˆpolicy onlyï¼‰
  2. `loss_v`ï¼ˆvalue onlyï¼‰
  3. `loss_pi + loss_v`ï¼ˆAlphaZeroãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
  4. `loss_pi Ã— loss_v`ï¼ˆä¹—ç®—ï¼‰

**çµæœ**:

| æå¤±é–¢æ•° | 6x6 Othello Elo | Connect Four Elo | ç·åˆè©•ä¾¡ |
|---------|-----------------|------------------|---------|
| loss_v | **æœ€é«˜** | **æœ€é«˜** | âœ… **å„ªå‹** |
| loss_pi + loss_v | 2ä½ | 2ä½ | âš ï¸ æ¨™æº–ã ãŒåŠ£ã‚‹ |
| loss_pi | ä¸‹ä½ | ä¸‹ä½ | âŒ å˜ç‹¬ã§ã¯å¼±ã„ |
| loss_pi Ã— loss_v | å¤‰å‹• | å¤‰å‹• | âš ï¸ ä¸å®‰å®š |

**é‡è¦ãªå¼•ç”¨**:
> "For relatively simple games such as 6Ã—6 Othello and Connect Four, optimizing the sum as AlphaZero does performs consistently worse than other objectives, in particular by optimizing only the value loss."

> "The loss_v (value-only loss) achieved the highest tournament Elo rating, in contrast to what AlphaZero uses and in contrast to the defaults of AlphaZeroGeneral."

**è§£é‡ˆ**:
- **å°ã•ã„ã‚²ãƒ¼ãƒ ã§ã¯value headã ã‘ã§ååˆ†**
- Policy headã¯è£œåŠ©çš„ãªå½¹å‰²ï¼ˆMCTSã®priorï¼‰
- AlphaZeroã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã¯**å¤§ãã„ã‚²ãƒ¼ãƒ ï¼ˆGoã€Chessï¼‰å‘ã‘ã«æœ€é©åŒ–**ã•ã‚Œã¦ã„ã‚‹å¯èƒ½æ€§

#### 2.2 Blokus Duoã¸ã®é©ç”¨

**Blokus Duoã®ç‰¹å¾´**:
- ãƒœãƒ¼ãƒ‰ã‚µã‚¤ã‚º: 14x14ï¼ˆ196ã‚»ãƒ«ï¼‰
- è¤‡é›‘æ€§: 6x6 Othelloï¼ˆ36ã‚»ãƒ«ï¼‰ã‚ˆã‚Šå¤§ãã„ãŒã€19x19 Goï¼ˆ361ã‚»ãƒ«ï¼‰ã‚ˆã‚Šå°ã•ã„
- åˆ†å²ä¿‚æ•°: åˆæ‰‹58æ‰‹ã€ä¸­ç›¤ä»¥é™æ¸›å°‘

**äºˆæ¸¬**:
- Wang & Emmerichã®**ã€Œå°ã•ã„ã‚²ãƒ¼ãƒ ã€ã‚«ãƒ†ã‚´ãƒªã«è©²å½“**ã™ã‚‹å¯èƒ½æ€§ãŒé«˜ã„
- Value-only trainingãŒæœ‰åŠ¹ã§ã‚ã‚‹ç¢ºç‡: **é«˜ã„**

#### 2.3 ç†è«–çš„èª¬æ˜

**ãªãœValue-onlyãŒæ©Ÿèƒ½ã™ã‚‹ã‹**:

1. **MCTSã®å½¹å‰²**
   - MCTSãŒvalueæ¨å®šã«åŸºã¥ã„ã¦æ¢ç´¢
   - Visit countã‹ã‚‰è‰¯ã„policyï¼ˆÏ€ï¼‰ã‚’ç”Ÿæˆ
   - Policy headã¯åˆæœŸpriorç¨‹åº¦ã®å½¹å‰²

2. **AlphaZeroã®æ§‹é€ **
   - Value head: ãƒ¡ã‚¤ãƒ³ã®æ„æ€æ±ºå®šï¼ˆQå€¤æ¨å®šï¼‰
   - Policy head: äºŒæ¬¡çš„ï¼ˆæ¢ç´¢ã®åˆæœŸãƒã‚¤ã‚¢ã‚¹ï¼‰
   - Value headãªã—ã§ã¯å®Œå…¨å´©å£Š
   - **Policy headãªã—ã§ã‚‚æ€§èƒ½ã¯å°‘ã—ä¸‹ãŒã‚‹ç¨‹åº¦**

3. **å°ã•ã„ã‚²ãƒ¼ãƒ ã®ç‰¹æ€§**
   - æ¢ç´¢ç©ºé–“ãŒæ¯”è¼ƒçš„å°ã•ã„
   - MCTSãŒååˆ†ã«æ¢ç´¢å¯èƒ½
   - æ­£ç¢ºãªvalueæ¨å®šãŒã‚ã‚Œã°æœ€é©æ‰‹ã‚’è¦‹ã¤ã‘ã‚‰ã‚Œã‚‹
   - Policy headã®äº‹å‰çŸ¥è­˜ã®é‡è¦æ€§ãŒä½ä¸‹

---

### 3. ä»–ã®é‡è¦ãªè¨“ç·´ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯

#### 3.1 KataGo: Policy Loss Scaling

**å®Ÿè£…**: [KataGo](https://github.com/lightvector/KataGo) - æœ€å¼·ã®GoAIå®Ÿè£…ã®ä¸€ã¤

**æå¤±é–¢æ•°**:
```python
loss = c_g * policy_loss + value_loss + c_L2 * L2_penalty
```

**ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**:
- `c_g = 1.5`: Policy loss scaling constant
- `c_L2 = 3e-5`: L2æ­£å‰‡åŒ–

**åŠ¹æœ**:
- AlphaZeroã®50åˆ†ã®1ã®è¨ˆç®—é‡ã§åŒç­‰ä»¥ä¸Šã®æ€§èƒ½
- 27 V100 GPUs Ã— 19æ—¥ = 1.4 GPU-yearsï¼ˆAlphaZeroã¯70 GPU-yearsï¼‰

**å«æ„**:
- Policy lossã®**ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°**ãŒé‡è¦
- AlphaZeroã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼ˆ1:1ï¼‰ãŒæœ€é©ã¨ã¯é™ã‚‰ãªã„

#### 3.2 Entropy Regularization

**ç†è«–**:
- Policy distributionã®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã‚’æå¤±ã«è¿½åŠ 
- æ¢ç´¢ä¿ƒé€²ã€æ—©æœŸåæŸé˜²æ­¢

**å®Ÿè£…**:
```python
entropy = -sum(p * log(p))
loss = policy_loss + value_loss - alpha * entropy
```

**ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**:
- `alpha`: å°ã•ã„å€¤ï¼ˆ0.01-0.1ï¼‰
- è¨“ç·´ä¸­ã«æ¸›è¡°ã•ã›ã‚‹ã“ã¨ã‚‚

**åŠ¹æœ**:
- æ¢ç´¢ã¨åˆ©ç”¨ã®ãƒãƒ©ãƒ³ã‚¹
- è¨“ç·´ã®å®‰å®šåŒ–
- å±€æ‰€æœ€é©å›é¿

#### 3.3 Temperatureèª¿æ•´

**MCTS visitåˆ†å¸ƒã®èª¿æ•´**:
```python
# Temperature = 1.0: ç¢ºç‡çš„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆè¨“ç·´åˆæœŸï¼‰
# Temperature â†’ 0: Greedyã«è¿‘ã¥ãï¼ˆè¨“ç·´å¾ŒæœŸï¼‰

pi = visits^(1/T) / sum(visits^(1/T))
```

**åŠ¹æœ**:
- è¨“ç·´åˆæœŸ: å¤šæ§˜ãªãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
- è¨“ç·´å¾ŒæœŸ: æœ€è‰¯æ‰‹ã«é›†ä¸­

---

### 4. è‡ªå·±å¯¾æˆ¦å¼·åŒ–å­¦ç¿’ã®ä¸€èˆ¬çš„å•é¡Œã¨è§£æ±ºç­–

#### 4.1 è¨“ç·´ä¸å®‰å®šæ€§

**å•é¡Œ**:
- Replay bufferã®ã‚µã‚¤ã‚ºèª¿æ•´ãƒŸã‚¹
- Temperature schedulingã®ãƒã‚°
- é•æ³•æ‰‹ã®ç”Ÿæˆ
- Max length gamesã®æ‰±ã„

**è§£æ±ºç­–**:
- è¨“ç·´ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ç¶™ç¶šçš„ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°
- Illegal movesã€self-atari loopsã®æ¤œå‡º
- Value/Policy imbalanceã®è¿½è·¡

#### 4.2 æ¢ç´¢vsåˆ©ç”¨ã®ãƒãƒ©ãƒ³ã‚¹

**å•é¡Œ**:
- åˆ©ç”¨ã®ã¿ â†’ åŒã˜æˆ¦ç•¥ã«å›ºåŸ·ï¼ˆéå­¦ç¿’ï¼‰
- æ¢ç´¢ã®ã¿ â†’ éç”Ÿç”£çš„ãªæ‰‹ã«æ™‚é–“æµªè²»

**è§£æ±ºç­–**:
- Entropy regularization
- Dirichlet noiseã®è¿½åŠ ï¼ˆAlphaZeroã®æ‰‹æ³•ï¼‰
- é©åˆ‡ãªPUCT constant

#### 4.3 Self-playå“è³ªã®ä½ä¸‹

**å•é¡Œ**:
- å¼±ã„ç›¸æ‰‹ã¨ã®å¯¾æˆ¦ â†’ ã‚¹ã‚­ãƒ«å‘ä¸Šã›ãš
- è² ã®ãƒ«ãƒ¼ãƒ—: å¼±ã„ãƒãƒªã‚·ãƒ¼ â†’ ä½å“è³ªã‚²ãƒ¼ãƒ  â†’ ã•ã‚‰ã«å¼±ã

**è§£æ±ºç­–**:
- éå»ãƒ¢ãƒ‡ãƒ«ã¨ã®å¯¾æˆ¦ï¼ˆdiversityç¢ºä¿ï¼‰
- League trainingï¼ˆè¤‡æ•°ã®ç›¸æ‰‹ï¼‰
- Comprehensive criticï¼ˆç›¸æ‰‹ã®æƒ…å ±ã‚‚åˆ©ç”¨ï¼‰

---

## æˆ‘ã€…ã®å•é¡Œã¸ã®é©ç”¨

### ç¾çŠ¶ã®è¨ºæ–­

**ç—‡çŠ¶**:
1. Value head: ç›¸é–¢0.63é”æˆï¼ˆâœ… å­¦ç¿’æˆåŠŸï¼‰
2. Policy head: Greedyæˆ¦ç•¥ã‚’é¸ã°ãªã„ï¼ˆâŒ å­¦ç¿’å¤±æ•—ï¼‰
3. å®Ÿæˆ¦æ€§èƒ½: 0% vs Greedyï¼ˆâŒ å®Œå…¨å¤±æ•—ï¼‰

**è©¦è¡Œã—ãŸè¨­å®š**:

| value_loss_weight | Valueç›¸é–¢ | å®Ÿæˆ¦æ€§èƒ½ | è©•ä¾¡ |
|-------------------|-----------|---------|------|
| 1.0 | 0.06-0.63ï¼ˆIterä¾å­˜ï¼‰ | 0% vs Greedy | âŒ |
| 0.1 | 0.10ï¼ˆIter 10ï¼‰ | 0% vs Greedy | âŒ |
| 0.01 | 0.14 | 0% vs Greedy | âŒ |

**å…±é€šã®å•é¡Œ**: **Policy lossã®å­˜åœ¨**ãŒvalueå­¦ç¿’ã‚’å¦¨ã’ã‚‹ã‹ã€é€†ã«é©åˆ‡ãªpolicyå­¦ç¿’ã‚’é˜»å®³

### æ–‡çŒ®ã‹ã‚‰ã®ç¤ºå”†

#### ç¤ºå”†1: Value-only Trainingï¼ˆæœ€å„ªå…ˆï¼‰

**æ ¹æ‹ **: Wang & Emmerich (2019)

**ä»®èª¬**:
- Policy lossã‚’å®Œå…¨ã«é™¤å»
- Value headã ã‘ãŒæ­£ç¢ºãªæ¨å®šã‚’å­¦ç¿’
- MCTSãŒvalueæ¨å®šã§è‰¯ã„æ‰‹ã‚’æ¢ç´¢
- Policy headã¯å­¦ç¿’ã—ãªã„ãŒMCTS priorã¨ã—ã¦æ©Ÿèƒ½ï¼ˆã¾ãŸã¯uniformï¼‰

**å®Ÿè£…**:
```python
loss = value_loss  # policy_lossã‚’å®Œå…¨é™¤å¤–
```

**æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœ**:
- Valueç›¸é–¢ãŒã•ã‚‰ã«å‘ä¸Šï¼ˆ0.63 â†’ 0.8+ï¼‰
- MCTSã®æ¢ç´¢å“è³ªå‘ä¸Š
- å®Ÿæˆ¦æ€§èƒ½ã®æ”¹å–„ï¼ˆç‰¹ã«Randomã«å¯¾ã—ã¦ï¼‰

**ãƒªã‚¹ã‚¯**:
- Policy headãŒå…¨ãå­¦ç¿’ã—ãªã„
- ã—ã‹ã—è«–æ–‡ã«ã‚ˆã‚Œã°**å•é¡Œãªã„**ï¼ˆuniform priorã§ã‚‚æ€§èƒ½ä½ä¸‹ã¯å°ã•ã„ï¼‰

#### ç¤ºå”†2: Policy Loss Scalingï¼ˆä»£æ›¿æ¡ˆï¼‰

**æ ¹æ‹ **: KataGo

**å®Ÿè£…**:
```python
loss = 1.5 * policy_loss + value_loss
```

**æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœ**:
- Policy headã®å­¦ç¿’å¼·åŒ–
- Value headã¨ã®ãƒãƒ©ãƒ³ã‚¹æ”¹å–„

**ãƒªã‚¹ã‚¯**:
- KataGoã¯Goã§æœ€é©åŒ–ã•ã‚ŒãŸå€¤
- Blokusã§ã¯ç•°ãªã‚‹å¯èƒ½æ€§

#### ç¤ºå”†3: Entropy Regularizationï¼ˆè£œåŠ©çš„ï¼‰

**å®Ÿè£…**:
```python
entropy = -sum(p * log(p))
loss = policy_loss + value_loss - 0.01 * entropy
```

**æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœ**:
- æ¢ç´¢ã®å¤šæ§˜æ€§å‘ä¸Š
- å±€æ‰€æœ€é©å›é¿

#### ç¤ºå”†4: è¨“ç·´ã‚¤ãƒ³ãƒ•ãƒ©æ”¹å–„

**Replay buffer**:
- ç¾åœ¨: ç„¡åŠ¹ï¼ˆbuffer_size=0ï¼‰
- ææ¡ˆ: æœ‰åŠ¹åŒ–ï¼ˆbuffer_size=1000-5000ï¼‰
- åŠ¹æœ: Catastrophic forgettingé˜²æ­¢

**Early stopping**:
- ç¾åœ¨: ãªã—ï¼ˆIter 50ã§éå­¦ç¿’å´©å£Šï¼‰
- ææ¡ˆ: 3-5ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ”¹å–„ãªã—ã§åœæ­¢
- åŠ¹æœ: æœ€è‰¯ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ä¿å­˜

**Evaluationé »åº¦**:
- ç¾åœ¨: 5ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã”ã¨
- ææ¡ˆ: 5ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã”ã¨ï¼ˆç¶­æŒï¼‰+ è©³ç´°ãƒ¡ãƒˆãƒªã‚¯ã‚¹

---

## æ¨å¥¨å®Ÿé¨“è¨ˆç”»

### Phase 1: Value-Only Trainingï¼ˆæœ€å„ªå…ˆï¼‰âš¡

**è¨­å®š**:
```python
from blokus_ai.train import main

main(
    num_iterations=40,
    games_per_iteration=5,
    num_simulations=200,           # Iter 40ã®æˆåŠŸè¨­å®š
    eval_interval=5,
    eval_games=20,
    past_generations=[5, 10],

    # â˜… é‡è¦å¤‰æ›´
    value_loss_weight=1.0,
    policy_loss_weight=0.0,        # â˜… Policy lossã‚’å®Œå…¨é™¤å¤–

    buffer_size=0,                 # ã¾ãšã‚·ãƒ³ãƒ—ãƒ«ã«
    batch_size=32,
    num_training_steps=100,
    learning_rate=5e-4,            # Iter 40ã®æˆåŠŸè¨­å®š
    max_grad_norm=1.0,
    use_lr_scheduler=False,
)
```

**æˆåŠŸåŸºæº–**:
- âœ… Valueç›¸é–¢ > 0.6ï¼ˆç¶­æŒã¾ãŸã¯æ”¹å–„ï¼‰
- âœ… AI vs Random > 40%ï¼ˆç¾åœ¨25%ã‹ã‚‰æ”¹å–„ï¼‰
- âœ… AI vs Greedy > 10%ï¼ˆç¾åœ¨0%ã‹ã‚‰æ”¹å–„ï¼‰

**ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³**: ~3-5æ™‚é–“ï¼ˆ40ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰

### Phase 2: Policy Scalingï¼ˆPhase 1å¤±æ•—æ™‚ï¼‰

**è¨­å®š**:
```python
main(
    ...
    value_loss_weight=1.0,
    policy_loss_weight=1.5,        # â˜… KataGo style
    ...
)
```

### Phase 3: Hybridï¼ˆä¸¡æ–¹å¤±æ•—æ™‚ï¼‰

**è¨­å®š**:
```python
main(
    ...
    value_loss_weight=1.0,
    policy_loss_weight=0.01,       # â˜… æ¥µå°ã®policy loss
    entropy_regularization=0.01,   # â˜… Entropyè¿½åŠ ï¼ˆè¦å®Ÿè£…ï¼‰
    ...
)
```

### Phase 4: Imitation Learningï¼ˆå…¨ã¦å¤±æ•—æ™‚ï¼‰

**æˆ¦ç•¥**: Greedyæˆ¦ç•¥ã‚’äº‹å‰å­¦ç¿’ã€ãã®å¾Œself-play

---

## é–¢é€£ç ”ç©¶ãƒ»å®Ÿè£…ãƒªã‚½ãƒ¼ã‚¹

### é‡è¦è«–æ–‡

1. **Wang & Emmerich (2019)**: ["Policy or Value? Loss Function and Playing Strength in AlphaZero-like Self-play"](https://liacs.leidenuniv.nl/~plaata1/papers/CoG2019.pdf)
   - **æœ€é‡è¦**: Value-only trainingã®å®Ÿè¨¼

2. **Wu (2019)**: ["Accelerating Self-Play Learning in Go"](https://arxiv.org/pdf/1902.10565)
   - KataGoã®æ‰‹æ³•è©³ç´°

3. **Silver et al. (2017)**: ["Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm"](https://www.science.org/doi/10.1126/science.aar6404)
   - AlphaZeroåŸè«–æ–‡

4. **Zhao et al. (2022)**: ["Efficient Learning for AlphaZero via Path Consistency"](https://proceedings.mlr.press/v162/zhao22h/zhao22h.pdf)
   - åŠ¹ç‡çš„å­¦ç¿’æ‰‹æ³•

### å®Ÿè£…ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹

1. **KataGo**: [lightvector/KataGo](https://github.com/lightvector/KataGo)
   - æœ€å¼·ã®Goå®Ÿè£…ã€æœ€é©åŒ–ã•ã‚ŒãŸæå¤±é–¢æ•°

2. **AlphaZero.jl**: [jonathan-laurent/AlphaZero.jl](https://github.com/jonathan-laurent/AlphaZero.jl)
   - æ˜ç¢ºãªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å‚è€ƒ

3. **LightZero**: [opendilab/LightZero](https://github.com/opendilab/LightZero)
   - MCTS benchmarkãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯

4. **Blokus-RL**: [KubiakJakub01/Blokus-RL](https://github.com/KubiakJakub01/Blokus-RL)
   - Blokuså°‚ç”¨ã€å‚è€ƒå®Ÿè£…

### æ•™è‚²ãƒªã‚½ãƒ¼ã‚¹

1. **Simple Alpha Zero**: [suragnair.github.io/posts/alphazero.html](https://suragnair.github.io/posts/alphazero.html)
   - ã‚ã‹ã‚Šã‚„ã™ã„è§£èª¬

2. **AlphaZero Chessprogramming wiki**: [chessprogramming.org/AlphaZero](https://www.chessprogramming.org/AlphaZero)
   - è©³ç´°ãªæŠ€è¡“æƒ…å ±

---

## çµè«–

### ä¸»è¦ãªç™ºè¦‹

1. **Value-only training**ãŒå°ã•ã„ã‚²ãƒ¼ãƒ ã§å®Ÿè¨¼æ¸ˆã¿ï¼ˆWang & Emmerich 2019ï¼‰
2. Blokus Duoã¯ã€Œå°ã•ã„ã‚²ãƒ¼ãƒ ã€ã‚«ãƒ†ã‚´ãƒªã«è©²å½“ã™ã‚‹å¯èƒ½æ€§ãŒé«˜ã„
3. æˆ‘ã€…ã®ç—‡çŠ¶ï¼ˆValueå­¦ç¿’æˆåŠŸã€Policyå­¦ç¿’å¤±æ•—ã€å®Ÿæˆ¦å¤±æ•—ï¼‰ã¨å®Œå…¨ã«åˆè‡´
4. AlphaZeroã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã¯å¤§ãã„ã‚²ãƒ¼ãƒ å‘ã‘

### å³åº§ã®è¡Œå‹•

**Value-only trainingï¼ˆpolicy_loss_weight=0.0ï¼‰ã‚’ä»Šã™ãå®Ÿæ–½**

**ç†ç”±**:
- âœ… æŸ»èª­æ¸ˆã¿è«–æ–‡ã§å®Ÿè¨¼
- âœ… æˆ‘ã€…ã®å•é¡Œã«ç†è«–çš„ã«åˆè‡´
- âœ… å®Ÿè£…ãŒç°¡å˜ï¼ˆ1è¡Œå¤‰æ›´ï¼‰
- âœ… ãƒªã‚¹ã‚¯ãŒä½ã„ï¼ˆworst case: ç¾çŠ¶ç¶­æŒï¼‰
- âœ… è¨ˆç®—ã‚³ã‚¹ãƒˆã‚‚æ—¢å­˜ã¨åŒã˜

### é•·æœŸçš„å±•æœ›

Value-only trainingãŒæˆåŠŸã—ãŸå ´åˆ:
1. Early stoppingè¿½åŠ ï¼ˆéå­¦ç¿’é˜²æ­¢ï¼‰
2. Replay bufferæœ‰åŠ¹åŒ–ï¼ˆå®‰å®šåŒ–ï¼‰
3. ã‚ˆã‚Šé•·æœŸã®è¨“ç·´ï¼ˆ60-100ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
4. Pentobiã‚¨ãƒ³ã‚¸ãƒ³ã¨ã®å¯¾æˆ¦è©•ä¾¡

å¤±æ•—ã—ãŸå ´åˆ:
1. Policy scalingè©¦è¡Œ
2. Entropy regularizationè¿½åŠ 
3. Imitation learningï¼ˆæœ€çµ‚æ‰‹æ®µï¼‰

### æœŸå¾…ã•ã‚Œã‚‹æˆæœ

**ä¿å®ˆçš„äºˆæ¸¬**:
- AI vs Random: 25% â†’ **50%**ï¼ˆæœŸå¾…å€¤åˆ°é”ï¼‰
- AI vs Greedy: 0% â†’ **15-30%**ï¼ˆGreedy baselineãƒ¬ãƒ™ãƒ«ï¼‰

**æ¥½è¦³çš„äºˆæ¸¬**:
- AI vs Random: 25% â†’ **60-70%**ï¼ˆå„ªä½ï¼‰
- AI vs Greedy: 0% â†’ **40-50%**ï¼ˆGreedyã‚’è¶…ãˆã‚‹ï¼‰

**æ ¹æ‹ **:
- Wang & Emmerichã§Value-onlyãŒ**æœ€é«˜Elo**é”æˆ
- æˆ‘ã€…ã®Iter 40ã¯æ—¢ã«Valueç›¸é–¢0.63ï¼ˆååˆ†é«˜ã„ï¼‰
- MCTSãŒé©åˆ‡ã«æ©Ÿèƒ½ã™ã‚Œã°Greedyä»¥ä¸Šã®æ€§èƒ½ãŒæœŸå¾…ã§ãã‚‹

---

**Status**: ğŸ“Š èª¿æŸ»å®Œäº†ã€Value-only trainingæº–å‚™å®Œäº†
**Next**: Experiment Phase 1å®Ÿè¡Œ
